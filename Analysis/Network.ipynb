{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import CellModeller\n",
    "from scipy.interpolate import interp1d\n",
    "plt.rcParams['figure.figsize'] = 7, 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continuous model\n",
    "def step(p1, p2, p3, gamma, mu, d, e, n, Dt):\n",
    "    # Update protein concs\n",
    "    nextp1 = p1 + ((d + e*(p3**n))/(1 + p3**n) - gamma*p1 - mu*p1) * Dt\n",
    "    nextp2 = p2 + ((d + e*(p1**n))/(1 + p1**n) - gamma*p2 - mu*p2) * Dt\n",
    "    nextp3 = p3 + ((d + e*(p2**n))/(1 + p2**n) - gamma*p3 - mu*p3) * Dt\n",
    "    return nextp1, nextp2, nextp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gillespie's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build lineage's graph from last pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Deg03 to check correct values of \n",
    "path = '../Data/'\n",
    "model = 'simpleGrowth10/'\n",
    "files = os.listdir(path+model)\n",
    "files.sort()\n",
    "# not using module (think on removing it)\n",
    "#files = files[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get last pickle's lineage\n",
    "last_pickle = pickle.load(open(path+model+files[-1], 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing data\n",
    "lin = last_pickle['lineage']\n",
    "nodes = list(lin.keys())\n",
    "edges = [(v,k) for k,v in lin.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directed graph\n",
    "G = nx.DiGraph()\n",
    "# add cell's ids as nodes\n",
    "G.add_node(1)\n",
    "G.add_nodes_from(nodes)\n",
    "# add (parent_id, child_id) tuples as edges\n",
    "G.add_edges_from(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just testing\n",
    "list(G.successors(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop over pickles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sim params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuous model params\n",
    "Dt = 0.05\n",
    "gamma = 0.3\n",
    "d = 198.\n",
    "e = 0\n",
    "n = 2\n",
    "init_conds = [0, 0, 5.0]\n",
    "\n",
    "# Sim params\n",
    "pickeSteps = 1\n",
    "#time between pickles \n",
    "p_time = Dt*pickeSteps #hours\n",
    "#store_folder = path+model[:-1]+'SIM2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# database file structure\n",
    "\"\"\"\n",
    "database = \n",
    "{\n",
    "    'it': {\n",
    "        'cell_id1':\n",
    "        {'pos': [x, y, z], 'fluo': [p1, p2, p3]},\n",
    "        'cell_id2':\n",
    "        {'pos': [x, y, z], 'fluo': [p1, p2, p3]},\n",
    "        ...\n",
    "    },\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize pickle 0 with desired initial conditions\n",
    "pickle_0 = pickle.load(open(path+model+'step-00000.pickle', 'rb'))\n",
    "pickle_0['cellStates'][1].color = init_conds\n",
    "pos = pickle_0['cellStates'][1].pos\n",
    "pos = [float(i) for i in pos]\n",
    "\n",
    "database = {}\n",
    "database[0] = {1: {'pos': pos, 'fluo': init_conds}}\n",
    "\n",
    "#for idx in range(1, len(files)):\n",
    "startproc = time.time()\n",
    "for idx in range(1, len(files)):\n",
    "    print(\"######################################################\")\n",
    "    print(\"idx: \", idx)\n",
    "    startwhole = time.time()\n",
    "    # get pickles lineage\n",
    "    # this pickle needs to have concs, so get it from pickle save past iteration\n",
    "    # Create new key in database:\n",
    "    database[idx] = {}\n",
    "    data1 = database[idx-1]\n",
    "    data2 = pickle.load(open(path+model+files[idx], 'rb'))\n",
    "    \n",
    "    print(f\"Number of cells: {len(data2['cellStates'].keys())}\")\n",
    "    \n",
    "    # need to identify which cells divided from one pickle to the next\n",
    "    # tells me which cells already divided when compared with previous pickle\n",
    "    # will use: data['cellStates'].keys()\n",
    "\n",
    "    # cells present in 2 but not in 1 (children)\n",
    "    not_in_prev = list(set(data2['cellStates'].keys()) - set(data1.keys()))    \n",
    "    \n",
    "    # cell division\n",
    "    if len(not_in_prev) > 0:\n",
    "        # cells in 1 and not in 2 (parents)\n",
    "        not_in_curr = list(set(data1.keys()) - set(data2['cellStates'].keys()))\n",
    "        \n",
    "        # each cell in not_in_curr (parents)\n",
    "        for par_cell in not_in_curr:       \n",
    "            succ = list(G.successors(par_cell))\n",
    "            \n",
    "            database[idx][succ[0]] = {}\n",
    "            database[idx][succ[1]] = {}\n",
    "            \n",
    "            # SPLIT CONC from parent\n",
    "            # 1) get conc from picke 1      \n",
    "            concs_1 = data1[par_cell]['fluo']\n",
    "            # 2) split conc in 2\n",
    "            new_conc = np.array(concs_1)\n",
    "            \n",
    "            # 3) assign pos, conc to each child\n",
    "            # Assign pos            \n",
    "            # Needs to convert from numpy.float32 to float\n",
    "            database[idx][succ[0]]['pos'] = [float(p) for p in data2['cellStates'][succ[0]].pos]\n",
    "            database[idx][succ[1]]['pos'] = [float(p) for p in data2['cellStates'][succ[1]].pos]\n",
    "            \n",
    "            # Assign species conc\n",
    "            database[idx][succ[0]]['fluo'] = new_conc.tolist()\n",
    "            database[idx][succ[1]]['fluo'] = new_conc.tolist()\n",
    "                    \n",
    "            # cells remaining in not_in_prev obj\n",
    "            new_left = list(set(not_in_prev) - set(succ))\n",
    "                    \n",
    "            # update not_in_prev\n",
    "            not_in_prev = new_left\n",
    "            \n",
    "    # no cell division\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    ##################################\n",
    "    # RUN CONTINUOUS OR GILLESPIE SIM\n",
    "    ##################################\n",
    "    \n",
    "    # this is for cells that did't get divided\n",
    "    #step(p1, p2, p3, gamma, mu, d, e, n, Dt)\n",
    "    \n",
    "    cells_both = list(set(data2['cellStates'].keys()) & set(data1.keys()))\n",
    "    start = time.time()\n",
    "    # FOR EACH CELL\n",
    "    for c in cells_both:\n",
    "        database[idx][c] = {}\n",
    "        # calculate concentrations\n",
    "        pos_2 = [float(p) for p in data2['cellStates'][c].pos]\n",
    "        concs = data1[c]['fluo']\n",
    "        \n",
    "        p1, p2, p3 = concs[0], concs[1], concs[2]\n",
    "        mu = float(data2['cellStates'][c].effGrowth)                \n",
    "        nextp1, nextp2, nextp3 = step(p1, p2, p3, gamma, mu, d, e, n, Dt)\n",
    "        \n",
    "        # assign to next pickle\n",
    "        database[idx][c]['pos'] = pos_2\n",
    "        database[idx][c]['fluo'] = [nextp1, nextp2, nextp3]                \n",
    "    \n",
    "    end = time.time()\n",
    "    print(f\"SIM: {end-start} secs\")\n",
    "    # STORE PICKLE WITH NEW CONCS\n",
    "        ## Send picke2 (data2) to be read and stored\n",
    "    # needs to have all concs updated\n",
    "    \n",
    "    endwhole = time.time()\n",
    "    print(f\"Time taken: {endwhole-startwhole}\")\n",
    "    \n",
    "endproc = time.time()\n",
    "print(f\"Whole process took {endproc-startproc} secs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "with open('data_contSIM100k.json', 'w') as outfile:  \n",
    "    json.dump(database, outfile)\n",
    "end = time.time()\n",
    "print(f\"saving took {end-start} secs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "start = time.time()\n",
    "np.save(\"data_contSIM.npy\", database)\n",
    "end = time.time()\n",
    "print(f\"saving took: {end-start}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PENDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print number of cells: OK\n",
    "# accumulate time and print OK\n",
    "# timing of each section: sets, euler: OK\n",
    "# try call step() once with vals for all of the cells: not too much time so no necessary. OK\n",
    "# store pos, fluos in a numpy array instead of pickles: OK\n",
    "# CHANGE COLOR FOR CONCENTRATION\n",
    "# check problem when store json with dict keys as integers and loaded as strings\n",
    "\n",
    "# SIMS\n",
    "# for gamma = 1, change timestep from 0.05 (?): All kept dt = 0.05\n",
    "# figure out how many cells for each simulation: OK\n",
    "# do sims for gamma = 1, 5, 10\n",
    "# change names to pickles, instead of timestep, put number of cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## When I store a file with dict keys as integers, they come as strings when loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_contSIM50k.json') as json_file:  \n",
    "    database = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "database_kymo = []\n",
    "width = 5\n",
    "for i in np.arange(10, len(database), 10):\n",
    "    # when used from database file generated in the code, so keys are integers\n",
    "    #data = database[i]\n",
    "    # when file is loaded, key are strings\n",
    "    data = database[str(i)]\n",
    "    \n",
    "    pos_x = [data[k]['pos'][0] for k in data.keys()]\n",
    "    pos_y = [data[k]['pos'][1] for k in data.keys()]\n",
    "    \n",
    "    xmax = math.ceil(max(list(map(abs, pos_x))))\n",
    "    ymax = math.ceil(max(list(map(abs, pos_y))))\n",
    "    \n",
    "    grid_size = max(xmax, ymax)\n",
    "    xx = np.arange(grid_size)\n",
    "    yy = np.arange(grid_size)\n",
    "    y,x = np.meshgrid(xx,yy)\n",
    "    c = grid_size / 2 - 1/2, grid_size / 2-1/2\n",
    "    r = np.sqrt((x-c[0])**2 + (y-c[1])**2)\n",
    "    \n",
    "    r_cells = np.array([np.sqrt(data[k]['pos'][0]**2 + data[k]['pos'][1]**2) for k in data.keys()])\n",
    "    R_cells = np.array([data[k]['fluo'][0] for k in data.keys()])\n",
    "    G_cells = np.array([data[k]['fluo'][1] for k in data.keys()])\n",
    "    B_cells = np.array([data[k]['fluo'][2] for k in data.keys()])\n",
    "    \n",
    "    nbins = int(r.max() // width)\n",
    "    bins_acc = []\n",
    "    R_acc = []\n",
    "    G_acc = []\n",
    "    B_acc = []\n",
    "    for dr in range(nbins):\n",
    "        bins_acc.append(dr*width)\n",
    "\n",
    "        idx = np.where((r_cells > dr*width)*(r_cells < (dr+1)*width))\n",
    "        R_acc.append(np.mean(R_cells[idx]))\n",
    "        G_acc.append(np.mean(G_cells[idx]))\n",
    "        B_acc.append(np.mean(B_cells[idx]))\n",
    "\n",
    "    database_kymo.append([bins_acc, R_acc, G_acc, B_acc])\n",
    "\n",
    "end = time.time()\n",
    "print(f\"creating kymo database took {end-start} secs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [d[0] for d in database_kymo]\n",
    "R = [d[1] for d in database_kymo]\n",
    "G = [d[2] for d in database_kymo]\n",
    "B = [d[3] for d in database_kymo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "kymo = np.zeros([len(database_kymo), len(x[-1]), 3])\n",
    "for i, x_vals in enumerate(x):\n",
    "    kymo[i, :len(x_vals), 0] = R[i]\n",
    "    kymo[i, :len(x_vals), 1] = G[i]\n",
    "    kymo[i, :len(x_vals), 2] = B[i]\n",
    "\n",
    "vals = kymo[:,:,0]\n",
    "kymo[:,:,0] = (vals-vals.min()) / (vals.max()-vals.min())\n",
    "\n",
    "vals = kymo[:,:,1]\n",
    "kymo[:,:,1] = (vals-vals.min()) / (vals.max()-vals.min())\n",
    "\n",
    "vals = kymo[:,:,2]\n",
    "kymo[:,:,2] = (vals-vals.min()) / (vals.max()-vals.min())\n",
    "\n",
    "end = time.time()\n",
    "print(f\"creating kymo database took {end-start} secs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(kymo);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_contSIM100k.json', 'w') as outfile:  \n",
    "    json.dump(kymo, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"kymo50k.npy\", kymo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
